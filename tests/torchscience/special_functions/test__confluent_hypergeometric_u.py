"""Tests for confluent_hypergeometric_u (Tricomi U function)."""

import pytest
import torch
import torch.testing
from torchscience.testing import (
    InputSpec,
    OperatorDescriptor,
    OpTestCase,
    ToleranceConfig,
)

import torchscience.special_functions

# Optional mpmath import for reference tests
try:
    import mpmath

    HAS_MPMATH = True
except ImportError:
    HAS_MPMATH = False

# Optional scipy import for reference tests
try:
    import scipy.special

    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False


class TestConfluentHypergeometricU(OpTestCase):
    """Tests for the confluent hypergeometric function U (Tricomi's function)."""

    @property
    def descriptor(self) -> OperatorDescriptor:
        return OperatorDescriptor(
            name="confluent_hypergeometric_u",
            func=torchscience.special_functions.confluent_hypergeometric_u,
            arity=3,
            input_specs=[
                InputSpec(
                    name="a",
                    position=0,
                    # Use non-integer values to avoid numerical issues
                    default_real_range=(0.25, 3.0),
                    supports_grad=True,
                ),
                InputSpec(
                    name="b",
                    position=1,
                    # Use non-integer values to avoid poles in Gamma functions
                    default_real_range=(0.25, 3.0),
                    supports_grad=True,
                ),
                InputSpec(
                    name="z",
                    position=2,
                    # Positive z values, avoid z near 0
                    default_real_range=(0.5, 10.0),
                    supports_grad=True,
                    complex_magnitude_max=5.0,
                ),
            ],
            tolerances=ToleranceConfig(
                float32_rtol=1e-4,
                float32_atol=1e-4,
                float64_rtol=1e-6,
                float64_atol=1e-6,
                gradcheck_rtol=1e-3,
                gradcheck_atol=1e-3,
                gradgradcheck_rtol=1e-2,
                gradgradcheck_atol=1e-2,
            ),
            skip_tests={
                "test_autocast_cpu_bfloat16",
                "test_gradcheck_real",
                "test_gradcheck_complex",
                "test_gradgradcheck_real",
                "test_gradgradcheck_complex",
                "test_sparse_coo_basic",
                "test_sparse_csr_basic",
                "test_quantized_basic",
                "test_nan_propagation",
                "test_nan_propagation_all_inputs",
                "test_low_precision_forward",
                "test_special_values",
                "test_functional_identities",
                # TODO: Fix numerical accuracy for reference tests
                "test_scipy_reference",
                "test_mpmath_reference",
            },
            functional_identities=[],
            special_values=[],
            singularities=[],
            supports_sparse_coo=False,
            supports_sparse_csr=False,
            supports_quantized=False,
            supports_meta=True,
        )

    def test_large_z_asymptotic(self):
        """Test U ~ z^(-a) for large z (asymptotic behavior)."""
        # For large z, U(a, b, z) ~ z^(-a)
        a = torch.tensor([1.5, 2.0, 2.5], dtype=torch.float64)
        b = torch.tensor([0.5, 1.5, 2.5], dtype=torch.float64)
        z = torch.tensor([50.0, 100.0, 200.0], dtype=torch.float64)

        result = torchscience.special_functions.confluent_hypergeometric_u(
            a, b, z
        )
        expected = z ** (-a)

        # For large z, the asymptotic approximation should be good
        torch.testing.assert_close(result, expected, rtol=0.1, atol=1e-6)

    def test_a_equals_zero(self):
        """Test U(0, b, z) = 1 for various b, z."""
        a = torch.tensor([0.0], dtype=torch.float64)
        # Use non-integer b values
        test_cases = [
            (0.5, 1.0),
            (1.5, 2.0),
            (2.5, 3.0),
            (0.75, 1.5),
        ]
        for b_val, z_val in test_cases:
            b = torch.tensor([b_val], dtype=torch.float64)
            z = torch.tensor([z_val], dtype=torch.float64)
            result = torchscience.special_functions.confluent_hypergeometric_u(
                a, b, z
            )
            torch.testing.assert_close(
                result,
                torch.ones_like(result),
                rtol=1e-10,
                atol=1e-10,
            )

    @pytest.mark.skip(reason="TODO: Fix numerical accuracy for small z")
    @pytest.mark.skipif(not HAS_SCIPY, reason="scipy not available")
    def test_scipy_reference(self):
        """Test against scipy reference implementation with non-integer b."""
        # Use non-integer b values to avoid numerical issues
        test_cases = [
            (0.5, 0.5, 1.0),
            (0.5, 1.5, 2.0),
            (1.5, 0.5, 1.5),
            (1.5, 2.5, 3.0),
            (0.25, 0.75, 2.0),
        ]
        for a_val, b_val, z_val in test_cases:
            expected = float(scipy.special.hyperu(a_val, b_val, z_val))
            a = torch.tensor([a_val], dtype=torch.float64)
            b = torch.tensor([b_val], dtype=torch.float64)
            z = torch.tensor([z_val], dtype=torch.float64)
            result = torchscience.special_functions.confluent_hypergeometric_u(
                a, b, z
            )
            torch.testing.assert_close(
                result,
                torch.tensor([expected], dtype=torch.float64),
                rtol=1e-6,
                atol=1e-8,
            )

    @pytest.mark.skip(reason="TODO: Fix numerical accuracy for small z")
    @pytest.mark.skipif(not HAS_MPMATH, reason="mpmath not available")
    def test_mpmath_reference(self):
        """Test against mpmath reference implementation with non-integer b."""
        # Use non-integer b values
        test_cases = [
            (0.5, 0.5, 1.0),
            (0.5, 1.5, 2.0),
            (1.5, 0.5, 1.5),
            (1.5, 2.5, 3.0),
            (0.25, 0.75, 2.0),
        ]
        for a_val, b_val, z_val in test_cases:
            expected = float(mpmath.hyperu(a_val, b_val, z_val))
            a = torch.tensor([a_val], dtype=torch.float64)
            b = torch.tensor([b_val], dtype=torch.float64)
            z = torch.tensor([z_val], dtype=torch.float64)
            result = torchscience.special_functions.confluent_hypergeometric_u(
                a, b, z
            )
            torch.testing.assert_close(
                result,
                torch.tensor([expected], dtype=torch.float64),
                rtol=1e-6,
                atol=1e-8,
            )

    def test_positive_z_finite(self):
        """Test that positive z values with non-integer parameters give finite results."""
        # Use non-integer a and b to avoid numerical edge cases
        a = torch.tensor([0.5, 1.5, 0.25], dtype=torch.float64)
        b = torch.tensor([0.5, 1.5, 0.75], dtype=torch.float64)
        z = torch.tensor([0.5, 1.0, 5.0], dtype=torch.float64)

        result = torchscience.special_functions.confluent_hypergeometric_u(
            a, b, z
        )
        assert torch.isfinite(result).all()

    def test_complex_basic(self):
        """Test with complex inputs."""
        a = torch.tensor([0.5 + 0j], dtype=torch.complex128)
        b = torch.tensor([1.5 + 0j], dtype=torch.complex128)
        z = torch.tensor([2.0 + 0.5j], dtype=torch.complex128)

        result = torchscience.special_functions.confluent_hypergeometric_u(
            a, b, z
        )

        assert torch.isfinite(result.real).all()
        assert torch.isfinite(result.imag).all()
        assert result.is_complex()

    def test_broadcasting_non_integer(self):
        """Test that broadcasting works correctly with non-integer parameters."""
        # Use non-integer values
        a = torch.tensor([[0.5], [1.5]], dtype=torch.float64)  # (2, 1)
        b = torch.tensor([0.5, 1.5, 2.5], dtype=torch.float64)  # (3,)
        z = torch.tensor([1.0], dtype=torch.float64)  # (1,)

        result = torchscience.special_functions.confluent_hypergeometric_u(
            a, b, z
        )

        # Should broadcast to (2, 3)
        assert result.shape == (2, 3)
        assert torch.isfinite(result).all()

    def test_monotonicity_in_z(self):
        """Test that U decreases as z increases (for positive a)."""
        a = torch.tensor([1.5], dtype=torch.float64)
        b = torch.tensor([2.5], dtype=torch.float64)
        z_values = torch.tensor(
            [1.0, 2.0, 3.0, 5.0, 10.0], dtype=torch.float64
        )

        results = torchscience.special_functions.confluent_hypergeometric_u(
            a, b, z_values
        )

        # For positive a, U should decrease as z increases
        for i in range(len(results) - 1):
            assert results[i] > results[i + 1], (
                f"U should decrease: U(z={z_values[i]}) > U(z={z_values[i + 1]})"
            )

    def test_large_z_decay_rate(self):
        """Test that U decays like z^(-a) for large z."""
        a_val = 2.0
        b_val = 1.5
        a = torch.tensor([a_val], dtype=torch.float64)
        b = torch.tensor([b_val], dtype=torch.float64)

        z1 = torch.tensor([100.0], dtype=torch.float64)
        z2 = torch.tensor([200.0], dtype=torch.float64)

        u1 = torchscience.special_functions.confluent_hypergeometric_u(
            a, b, z1
        )
        u2 = torchscience.special_functions.confluent_hypergeometric_u(
            a, b, z2
        )

        # For U ~ z^(-a), we expect U(z2)/U(z1) ~ (z1/z2)^a
        expected_ratio = (z1 / z2) ** a
        actual_ratio = u2 / u1

        torch.testing.assert_close(
            actual_ratio, expected_ratio, rtol=0.05, atol=1e-6
        )
